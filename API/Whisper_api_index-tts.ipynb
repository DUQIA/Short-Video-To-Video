{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0i8YHThLAzQ"
   },
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jCsicoVJLJo"
   },
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://github.com/index-tts/index-tts.git\n",
    "!cd /content/index-tts && git lfs pull\n",
    "!pip install -U uv\n",
    "!cd /content/index-tts && uv sync --all-extras\n",
    "\n",
    "!uv tool install \"huggingface-hub[cli,hf_xet]\"\n",
    "!hf download IndexTeam/IndexTTS-2 --local-dir=/content/index-tts/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIcYDtSfTqIo"
   },
   "outputs": [],
   "source": [
    "!cd /content/index-tts && uv python install 3.11 && uv tool install spleeter\n",
    "!wget -O /content/index-tts/2stems.tar.gz https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynTt2T_BUP7v"
   },
   "source": [
    "## API Cloudflare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1884,
     "status": "ok",
     "timestamp": 1768232472013,
     "user": {
      "displayName": "å¤•èŽ«",
      "userId": "11912185600167937708"
     },
     "user_tz": -480
    },
    "id": "vMXmxjNuUMeY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "API = userdata.get('API')\n",
    "TOKEN = userdata.get('TOKEN')\n",
    "\n",
    "os.environ['CLOUDFLARE_API'] = API\n",
    "os.environ['CLOUDFLARE_TOKEN'] = TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVu_AWQjnu1o"
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyE2NVrLnueE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "with open('index-tts/api.py', 'w') as f:\n",
    "  f.write(\n",
    "      '''\n",
    "import os\n",
    "import re\n",
    "import ffmpy\n",
    "import shutil\n",
    "import tarfile\n",
    "import requests\n",
    "import subprocess\n",
    "import gradio as gr\n",
    "from base64 import b64encode\n",
    "from indextts.infer_v2 import IndexTTS2\n",
    "if not os.path.exists('pretrained_models'):\n",
    "    with tarfile.open('2stems.tar.gz', 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall('./pretrained_models/2stems')\n",
    "API = os.environ.get('CLOUDFLARE_API')\n",
    "TOKEN = os.environ.get('CLOUDFLARE_TOKEN')\n",
    "API_BASE_URL = f\"https://api.cloudflare.com/client/v4/accounts/{API}/ai/run/\"\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "tts = IndexTTS2(cfg_path=\"/content/index-tts/checkpoints/config.yaml\", model_dir=\"checkpoints\", use_fp16=False, use_cuda_kernel=False, use_deepspeed=False)\n",
    "file_name = 'audio'\n",
    "folder = 'output_directory'\n",
    "main_video = f'./{folder}/video.mp4'\n",
    "main_audio = f'./{folder}/{file_name}.wav'\n",
    "vocals = f'./{folder}/{file_name}/vocals.wav'\n",
    "vocals_monorail = f'./{folder}/{file_name}/vocals_monorail.wav'\n",
    "voice_imitation = f'./{folder}/{file_name}/voice_imitation.wav'\n",
    "splitting_path = f'./{folder}/{file_name}/splitting/audio_splitting'\n",
    "translated_path = f'./{folder}/{file_name}/translated/audio_translated'\n",
    "mute = f'./{folder}/{file_name}/mute/'\n",
    "accompaniment = f'./{folder}/{file_name}/accompaniment.wav'\n",
    "output_audio = f'./{folder}/output.wav'\n",
    "output_video = f'./{folder}/output.mp4'\n",
    "def create_directory():\n",
    "  os.mkdir('output_directory')\n",
    "  os.mkdir(f'output_directory/{file_name}')\n",
    "  os.mkdir(f'output_directory/{file_name}/splitting')\n",
    "  os.mkdir(f'output_directory/{file_name}/translated')\n",
    "  os.mkdir(f'output_directory/{file_name}/mute')\n",
    "def ffmpeg_d(input, output):\n",
    "  ff = ffmpy.FFmpeg(\n",
    "  inputs = input,\n",
    "  outputs = output\n",
    "  )\n",
    "  ff.run()\n",
    "def gain_time(audio):\n",
    "  command = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', audio]\n",
    "  result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "  return result.stdout.decode().strip()\n",
    "\n",
    "def left_justified(audio):\n",
    "  try:\n",
    "    command = ['ffmpeg', '-i', audio, '-af', 'silencedetect=n=-38dB:d=0.01', '-f', 'null', '-']\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    start_justified = re.search(r'silence_duration: (\\d.\\d+)', result.stdout.decode(), re.M|re.S).group(1)\n",
    "  except AttributeError:\n",
    "    raise gr.Error('left_justified No start sound detected!')\n",
    "  return start_justified\n",
    "def right_justified(audio):\n",
    "  try:\n",
    "    command = ['ffmpeg', '-i', audio, '-af', 'areverse,silencedetect=n=-38dB:d=0.01,areverse', '-f', 'null', '-']\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    end_justified = re.search(r'silence_duration: (\\d.\\d+)', result.stdout.decode(), re.M|re.S).group(1)\n",
    "  except AttributeError:\n",
    "    raise gr.Error('right_justified No start sound detected!')\n",
    "  return end_justified\n",
    "def time_verify(vocals_audio, target_audio):\n",
    "  audios = [vocals_audio, target_audio]\n",
    "  justified = []\n",
    "  time_lists = []\n",
    "  for audio in audios:\n",
    "    justified.append(left_justified(audio))\n",
    "    time_lists.append(gain_time(audio))\n",
    "  j_time = float(justified[0]) - float(justified[1])\n",
    "  if float(time_lists[0]) > float(time_lists[1]):\n",
    "      r_time = float(min(time_lists)) / (float(max(time_lists)) - j_time)\n",
    "  else:\n",
    "      r_time = float(max(time_lists)) / float(min(time_lists))\n",
    "  return round(j_time, 6), round(r_time, 6)\n",
    "def run(model, inputs):\n",
    "  response = requests.post(f\"{API_BASE_URL}{model}\", headers=headers, json=inputs)\n",
    "  if response.status_code != 200:\n",
    "      raise gr.Error(f\"API call failed with status code {response.status_code}\")\n",
    "  return response.json()\n",
    "def audio_texts(audio):\n",
    "  time_period_list = list()\n",
    "  sound = open(audio, \"rb\").read()\n",
    "  sound_base64 = b64encode(sound).decode()\n",
    "  inputs = {\n",
    "      \"audio\": sound_base64,\n",
    "      \"task\": \"transcribe\"\n",
    "  }\n",
    "  output = run(\"@cf/openai/whisper-large-v3-turbo\", inputs)\n",
    "  language = output['result']['transcription_info']['language']\n",
    "  texts = output['result']['text']\n",
    "  vtt = output['result']['vtt']\n",
    "  print(language)\n",
    "  print(texts)\n",
    "  print(vtt)\n",
    "  for segment in output['result']['segments']:\n",
    "      time_period = dict({'start': segment['start'], 'end': segment['end'], 'text': segment['text']})\n",
    "      time_period_list.append(time_period)\n",
    "  for i in range(len(time_period_list)):\n",
    "    time_period_list[i]['end'] = float(gain_time(vocals_monorail)) if i == len(time_period_list) - 1 else time_period_list[i]['end']\n",
    "  print('âœ… time_period_list:', time_period_list)\n",
    "  return language, texts, vtt, time_period_list\n",
    "def translate_texts(texts, language):\n",
    "  ts_texts = list()\n",
    "  prompt = \"\"\"\n",
    "  You are a professional {{to}} native translator who needs to fluently translate text into {{to}}.\n",
    "\n",
    "  ## Translation Rules\n",
    "  1. Output only the translated content, without explanations or additional content (such as \"Here's the translation:\" or \"Translation as follows:\")\n",
    "  2. The returned translation must maintain exactly the same number of paragraphs and format as the original text\n",
    "  3. If the text contains HTML tags, consider where the tags should be placed in the translation while maintaining fluency\n",
    "  4. For content that should not be translated (such as proper nouns, code, etc.), keep the original text.\n",
    "  5. If input contains %%, use %% in your output, if input has no %%, don't use %% in your output{{title_prompt}}{{summary_prompt}}{{terms_prompt}}\n",
    "\n",
    "  ## OUTPUT FORMAT:\n",
    "  - **Single paragraph input** â†’ Output translation directly (no separators, no extra text)\n",
    "  - **Multi-paragraph input** â†’ Use %% as paragraph separator between translations\n",
    "\n",
    "  ## Examples\n",
    "  ### Multi-paragraph Input:\n",
    "  Paragraph A\n",
    "  %%\n",
    "  Paragraph B\n",
    "  %%\n",
    "  Paragraph C\n",
    "  %%\n",
    "  Paragraph D\n",
    "\n",
    "  ### Multi-paragraph Output:\n",
    "  Translation A\n",
    "  %%\n",
    "  Translation B\n",
    "  %%\n",
    "  Translation C\n",
    "  %%\n",
    "  Translation D\n",
    "\n",
    "  ### Single paragraph Input:\n",
    "  Single paragraph content\n",
    "\n",
    "  ### Single paragraph Output:\n",
    "  Direct translation without separators\n",
    "  \"\"\"\n",
    "  for text in range(len(texts)):\n",
    "    response = requests.post(\n",
    "    f\"https://api.cloudflare.com/client/v4/accounts/{API}/ai/v1/responses\",\n",
    "      headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "      json={\n",
    "        \"model\": \"@cf/openai/gpt-oss-120b\",\n",
    "        \"input\": f\"Translate to {language} ({texts[text]['text']})\"\n",
    "      }\n",
    "    )\n",
    "    result = response.json()\n",
    "    ts_texts.append(result['output'][1]['content'][0]['text'])\n",
    "  print('âœ… ts_texts:', ts_texts)\n",
    "  return ts_texts\n",
    "def audio_splitting(time_period_list):\n",
    "  for i in range(len(time_period_list)):\n",
    "    ffmpeg_d(\n",
    "        {vocals_monorail: None},\n",
    "        {\n",
    "        f'{splitting_path}_{i}.wav': ['-i', vocals_monorail, '-ss', f\"{time_period_list[i]['start']}\", '-to', f\"{time_period_list[i]['end']}\", '-codec', 'copy', '-f', 'wav']\n",
    "        }\n",
    "  )\n",
    "def audio_translate(time_period_list, ts_texts):\n",
    "  try:\n",
    "    for a in range(len(ts_texts)):\n",
    "      tts.infer(spk_audio_prompt=f'{splitting_path}_{a}.wav', text=ts_texts[a], output_path=f'{translated_path}_{a}.wav', verbose=True)\n",
    "  except Exception as e:\n",
    "    raise gr.Error(f'âœ… time_period_list: {time_period_list}\\\\n âœ… ts_texts: {ts_texts}\\\\n âš ï¸ {splitting_path}_{a}.wav and {time_period_list[a]} and {ts_texts[a]}\\\\n âŒ Error: {e}')\n",
    "def video_inputs(video, LANGUAGE):\n",
    "  if not os.path.exists('output_directory'):\n",
    "      create_directory()\n",
    "  else:\n",
    "    shutil.rmtree('output_directory')\n",
    "    create_directory()\n",
    "  ffmpeg_d(\n",
    "      {video: None},\n",
    "      {\n",
    "      main_video: ['-y', '-map', '0:0', '-c:a', 'copy', '-f', 'mp4'],\n",
    "      main_audio: ['-y', '-map', '0:a', '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', '-f', 'wav']\n",
    "      }\n",
    "  )\n",
    "  subprocess.run(['uv', 'tool', 'run', 'spleeter', 'separate', '-o', folder, '-p', 'spleeter:2stems-16kHz', main_audio])\n",
    "  ffmpeg_d(\n",
    "      {vocals: None},\n",
    "      {\n",
    "      vocals_monorail: ['-y', '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', '-f', 'wav']\n",
    "      }\n",
    "  )\n",
    "  language, texts, vtt, time_period_list = audio_texts(vocals_monorail)\n",
    "  audio_splitting(time_period_list)\n",
    "  ts_texts = translate_texts(time_period_list, LANGUAGE)\n",
    "  audio_translate(time_period_list, ts_texts)\n",
    "  message = list()\n",
    "  for j in range(len(time_period_list)):\n",
    "    ffmpeg_d(\n",
    "      {f'{translated_path}_{j}.wav': None},\n",
    "      {\n",
    "      f'{mute}left_audio_{j}.wav': ['-y', '-af', f\"atrim=start={left_justified(f'{translated_path}_{j}.wav')}\"]\n",
    "      }\n",
    "    )\n",
    "    ffmpeg_d(\n",
    "      {f'{mute}left_audio_{j}.wav': None},\n",
    "      {\n",
    "      f'{mute}right_audio_{j}.wav': ['-y', '-af', f\"areverse,atrim=start={right_justified(f'{translated_path}_{j}.wav')},areverse\"]\n",
    "      }\n",
    "    )\n",
    "    left_justified_time = float(left_justified(f'{splitting_path}_{j}.wav'))\n",
    "    if left_justified_time > 0:\n",
    "      ffmpeg_d(\n",
    "        {f'{mute}right_audio_{j}.wav': None},\n",
    "        {\n",
    "        f'{mute}output_left_audio_{j}.wav': ['-y', '-af', f'areverse,apad=pad_dur={left_justified_time}s,areverse']\n",
    "        }\n",
    "      )\n",
    "      message.append(f\"âœ… output_left_audio_{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {left_justified_time}\")\n",
    "    else:\n",
    "      ffmpeg_d(\n",
    "        {f'{mute}right_audio_{j}.wav': None},\n",
    "        {\n",
    "        f'{mute}output_left_audio_{j}.wav': ['-y',  '-filter:a', f'atrim=start={abs(left_justified_time)}']\n",
    "        }\n",
    "      )\n",
    "      message.append(f\"âš ï¸ output_left_audio_{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {left_justified_time}\")\n",
    "    r_time = time_verify(f'{splitting_path}_{j}.wav', f'{mute}output_left_audio_{j}.wav')\n",
    "    ffmpeg_d(\n",
    "      {f'{mute}output_left_audio_{j}.wav': None},\n",
    "      {\n",
    "      f'{mute}output_rate_audio_{j}.wav': ['-y', '-filter:a', f'atempo={r_time[1]}']\n",
    "      }\n",
    "    )\n",
    "    if j == 0:\n",
    "      gap_time = float(time_period_list[j+1]['start']) - float(gain_time(f'{mute}output_rate_audio_{j}.wav'))\n",
    "    if j > 0 and j == len(time_period_list) - 2:\n",
    "      gap_time = float(time_period_list[j+1]['start']) - (float(gain_time(f'{mute}output_gap_audio{j-1}.wav'))+float(gain_time(f'{mute}output_rate_audio_{j}.wav')))\n",
    "    elif j > 0 and j == len(time_period_list) - 1:\n",
    "      gap_time = 0\n",
    "    if gap_time < 0:\n",
    "      raise gr.Error(f'âš ï¸ gap_time is negative: {gap_time} at segment {j}. Please check the time periods and audio lengths.')\n",
    "    if gap_time > 0 and j == 0:\n",
    "      ffmpeg_d(\n",
    "        {f'{mute}output_rate_audio_{j}.wav': None},\n",
    "        {\n",
    "        f'{mute}output_gap_audio{j}.wav': ['-y', '-af', f'apad=pad_dur={gap_time}s']\n",
    "        }\n",
    "      )\n",
    "      message.append(f\"âœ… output_gap_audio{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {gap_time}\")\n",
    "    elif gap_time > 0 and j == len(time_period_list) - 2:\n",
    "      ffmpeg_d(\n",
    "        {\n",
    "        f'{mute}output_gap_audio{j-1}.wav': None,\n",
    "        f'{mute}output_rate_audio_{j}.wav': None\n",
    "        },\n",
    "        {\n",
    "        f'{mute}output_gap_audio{j}_temp.wav': ['-y', '-filter_complex', '[0:a][1:a]concat=n=2:v=0:a=1[outa]', '-map', '[outa]']\n",
    "        }\n",
    "      )\n",
    "      ffmpeg_d(\n",
    "        {f'{mute}output_gap_audio{j}_temp.wav': None},\n",
    "        {\n",
    "        f'{mute}output_gap_audio{j}.wav': ['-y', '-af', f'apad=pad_dur={gap_time}s']\n",
    "        }\n",
    "      )\n",
    "      message.append(f\"âœ… output_gap_audio{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {gap_time}\")\n",
    "    else:\n",
    "      ffmpeg_d(\n",
    "        {\n",
    "        f'{mute}output_gap_audio{j-1}.wav': None,\n",
    "        f'{mute}output_rate_audio_{j}.wav': None\n",
    "        },\n",
    "        {\n",
    "        voice_imitation: ['-y', '-filter_complex', '[0:a][1:a]concat=n=2:v=0:a=1[outa]', '-map', '[outa]']\n",
    "        }\n",
    "      )\n",
    "      message.append(f'âœ… output.wav: {time_period_list[j][\"start\"]}-{time_period_list[j][\"end\"]}')\n",
    "  ffmpeg_d(\n",
    "      {voice_imitation: None, accompaniment: None},\n",
    "      {\n",
    "      output_audio: ['-y', '-filter_complex', 'amix=inputs=2']\n",
    "      }\n",
    "  )\n",
    "  ffmpeg_d(\n",
    "      {output_audio: None, main_video: None},\n",
    "      {\n",
    "      output_video: ['-y', '-c:v', 'copy', '-c:a', 'aac', '-strict', 'experimental']\n",
    "      }\n",
    "  )\n",
    "  print('\\\\n'.join(message))\n",
    "  return output_video, vocals_monorail, voice_imitation, accompaniment, ts_texts, time_period_list, vtt\n",
    "with gr.Blocks() as demo:\n",
    "  LANGUAGE = gr.Text(value='Chinese (Simplified)', label='Language')\n",
    "  gr.Interface(\n",
    "    fn=video_inputs,\n",
    "    inputs=[\n",
    "        gr.Video(height=320, interactive=True, label='Input_video'),\n",
    "        LANGUAGE\n",
    "        ],\n",
    "    outputs=[\n",
    "        gr.Video(height=320, label='Output_video'),\n",
    "        gr.Audio(label='Vocals_monorail'),\n",
    "        gr.Audio(label='Voice_imitation'),\n",
    "        gr.Audio(label='Accompaniment'),\n",
    "        gr.Text(label='Translator'),\n",
    "        gr.Text(label='Time_period_list'),\n",
    "        gr.Text(label='VTT'),\n",
    "    ],\n",
    "    title=\"Short-Video-To-Video API\",\n",
    "    description=\"ðŸ¤— Use the API provided by [Cloudflare](https://developers.cloudflare.com/workers-ai/models/?authors=openai) to call Whisper, and then convert the audio locally through the index-tts function.\"\n",
    "  )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  demo.launch(share=True)\n",
    "      '''\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb7N2tIFUeC3"
   },
   "source": [
    "## script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33yZFfL2UfNZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "with open('index-tts/script.py', 'w') as f:\n",
    "  f.write(\n",
    "      '''\n",
    "import re\n",
    "import ffmpy\n",
    "import subprocess\n",
    "from indextts.infer_v2 import IndexTTS2\n",
    "file_name = 'audio'\n",
    "folder = 'output_directory'\n",
    "main_video = f'./{folder}/video.mp4'\n",
    "vocals = f'./{folder}/{file_name}/vocals.wav'\n",
    "voice_imitation = f'./{folder}/{file_name}/voice_imitation.wav'\n",
    "splitting_path = f'./{folder}/{file_name}/splitting/audio_splitting'\n",
    "translated_path = f'./{folder}/{file_name}/translated/audio_translated'\n",
    "mute = f'./{folder}/{file_name}/mute/'\n",
    "accompaniment = f'./{folder}/{file_name}/accompaniment.wav'\n",
    "output_audio = f'./{folder}/output.wav'\n",
    "output_video = f'./{folder}/output.mp4'\n",
    "\n",
    "# Modify parameters before running\n",
    "time_period_list = [{'start': 0, 'end': 4.06, 'text': 'xxx'}, {'start': 4.46, 'end': 9.28, 'text': 'xxx'}, {'start': 9.54, 'end': 15.325188, 'text': 'xxx'}]\n",
    "ts_texts = ['xxx', 'xxx', 'xxx']\n",
    "num = 1\n",
    "\n",
    "def gain_time(audio):\n",
    "  command = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', audio]\n",
    "  result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "  return result.stdout.decode().strip()\n",
    "def left_justified(audio):\n",
    "  try:\n",
    "    command = ['ffmpeg', '-i', audio, '-af', 'silencedetect=n=-38dB:d=0.01', '-f', 'null', '-']\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    start_justified = re.search(r'silence_duration: (\\d.\\d+)', result.stdout.decode(), re.M|re.S).group(1)\n",
    "  except AttributeError:\n",
    "    raise Exception('left_justified No start sound detected!')\n",
    "  return start_justified\n",
    "def right_justified(audio):\n",
    "  try:\n",
    "    command = ['ffmpeg', '-i', audio, '-af', 'areverse,silencedetect=n=-38dB:d=0.01,areverse', '-f', 'null', '-']\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    end_justified = re.search(r'silence_duration: (\\d.\\d+)', result.stdout.decode(), re.M|re.S).group(1)\n",
    "  except AttributeError:\n",
    "    raise Exception('right_justified No start sound detected!')\n",
    "  return end_justified\n",
    "def time_verify(vocals_audio, target_audio):\n",
    "  audios = [vocals_audio, target_audio]\n",
    "  justified = []\n",
    "  time_lists = []\n",
    "  for audio in audios:\n",
    "    justified.append(left_justified(audio))\n",
    "    time_lists.append(gain_time(audio))\n",
    "  j_time = float(justified[0]) - float(justified[1])\n",
    "  if float(time_lists[0]) > float(time_lists[1]):\n",
    "      r_time = float(min(time_lists)) / (float(max(time_lists)) - j_time)\n",
    "  else:\n",
    "      r_time = float(max(time_lists)) / float(min(time_lists))\n",
    "  return round(j_time, 6), round(r_time, 6)\n",
    "def audio_translate(ts_texts):\n",
    "  tts = IndexTTS2(cfg_path=\"/content/index-tts/checkpoints/config.yaml\", model_dir=\"checkpoints\", use_fp16=False, use_cuda_kernel=False, use_deepspeed=False)\n",
    "  for a in range(num, len(ts_texts)):\n",
    "    tts.infer(spk_audio_prompt=f'{splitting_path}_{a}.wav', text=ts_texts[a], output_path=f'{translated_path}_{a}.wav', verbose=True)\n",
    "def ffmpeg_d(input, output):\n",
    "  ff = ffmpy.FFmpeg(\n",
    "  inputs = input,\n",
    "  outputs = output\n",
    "  )\n",
    "  ff.run()\n",
    "audio_translate(ts_texts)\n",
    "message = list()\n",
    "for j in range(len(time_period_list)):\n",
    "  ffmpeg_d(\n",
    "    {f'{translated_path}_{j}.wav': None},\n",
    "    {\n",
    "    f'{mute}left_audio_{j}.wav': ['-y', '-af', f\"atrim=start={left_justified(f'{translated_path}_{j}.wav')}\"]\n",
    "    }\n",
    "  )\n",
    "  ffmpeg_d(\n",
    "    {f'{mute}left_audio_{j}.wav': None},\n",
    "    {\n",
    "    f'{mute}right_audio_{j}.wav': ['-y', '-af', f\"areverse,atrim=start={right_justified(f'{translated_path}_{j}.wav')},areverse\"]\n",
    "    }\n",
    "  )\n",
    "  left_justified_time = float(left_justified(f'{splitting_path}_{j}.wav'))\n",
    "  if left_justified_time > 0:\n",
    "    ffmpeg_d(\n",
    "      {f'{mute}right_audio_{j}.wav': None},\n",
    "      {\n",
    "      f'{mute}output_left_audio_{j}.wav': ['-y', '-af', f'areverse,apad=pad_dur={left_justified_time}s,areverse']\n",
    "      }\n",
    "    )\n",
    "    message.append(f\"âœ… output_left_audio_{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {left_justified_time}\")\n",
    "  else:\n",
    "    ffmpeg_d(\n",
    "      {f'{mute}right_audio_{j}.wav': None},\n",
    "      {\n",
    "      f'{mute}output_left_audio_{j}.wav': ['-y',  '-filter:a', f'atrim=start={abs(left_justified_time)}']\n",
    "      }\n",
    "    )\n",
    "    message.append(f\"âš ï¸ output_left_audio_{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {left_justified_time}\")\n",
    "  r_time = time_verify(f'{splitting_path}_{j}.wav', f'{mute}output_left_audio_{j}.wav')\n",
    "  ffmpeg_d(\n",
    "    {f'{mute}output_left_audio_{j}.wav': None},\n",
    "    {\n",
    "    f'{mute}output_rate_audio_{j}.wav': ['-y', '-filter:a', f'atempo={r_time[1]}']\n",
    "    }\n",
    "  )\n",
    "  if j == 0:\n",
    "    gap_time = float(time_period_list[j+1]['start']) - float(gain_time(f'{mute}output_rate_audio_{j}.wav'))\n",
    "  if j > 0 and j == len(time_period_list) - 2:\n",
    "    gap_time = float(time_period_list[j+1]['start']) - (float(gain_time(f'{mute}output_gap_audio{j-1}.wav'))+float(gain_time(f'{mute}output_rate_audio_{j}.wav')))\n",
    "  elif j > 0 and j == len(time_period_list) - 1:\n",
    "    gap_time = 0\n",
    "  if gap_time < 0:\n",
    "    raise Exception(f'âš ï¸ gap_time is negative: {gap_time} at segment {j}. Please check the time periods and audio lengths.')\n",
    "  if gap_time > 0 and j == 0:\n",
    "    ffmpeg_d(\n",
    "      {f'{mute}output_rate_audio_{j}.wav': None},\n",
    "      {\n",
    "      f'{mute}output_gap_audio{j}.wav': ['-y', '-af', f'apad=pad_dur={gap_time}s']\n",
    "      }\n",
    "    )\n",
    "    message.append(f\"âœ… output_gap_audio{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {gap_time}\")\n",
    "  elif gap_time > 0 and j == len(time_period_list) - 2:\n",
    "    ffmpeg_d(\n",
    "      {\n",
    "      f'{mute}output_gap_audio{j-1}.wav': None,\n",
    "      f'{mute}output_rate_audio_{j}.wav': None\n",
    "      },\n",
    "      {\n",
    "      f'{mute}output_gap_audio{j}_temp.wav': ['-y', '-filter_complex', '[0:a][1:a]concat=n=2:v=0:a=1[outa]', '-map', '[outa]']\n",
    "      }\n",
    "    )\n",
    "    ffmpeg_d(\n",
    "      {f'{mute}output_gap_audio{j}_temp.wav': None},\n",
    "      {\n",
    "      f'{mute}output_gap_audio{j}.wav': ['-y', '-af', f'apad=pad_dur={gap_time}s']\n",
    "      }\n",
    "    )\n",
    "    message.append(f\"âœ… output_gap_audio{j}.wav: {time_period_list[j]['start']}-{time_period_list[j]['end']} : {gap_time}\")\n",
    "  else:\n",
    "    ffmpeg_d(\n",
    "      {\n",
    "      f'{mute}output_gap_audio{j-1}.wav': None,\n",
    "      f'{mute}output_rate_audio_{j}.wav': None\n",
    "      },\n",
    "      {\n",
    "      voice_imitation: ['-y', '-filter_complex', '[0:a][1:a]concat=n=2:v=0:a=1[outa]', '-map', '[outa]']\n",
    "      }\n",
    "    )\n",
    "    message.append(f'âœ… output.wav: {time_period_list[j][\"start\"]}-{time_period_list[j][\"end\"]}')\n",
    "ffmpeg_d(\n",
    "    {voice_imitation: None, accompaniment: None},\n",
    "    {\n",
    "    output_audio: ['-y', '-filter_complex', 'amix=inputs=2']\n",
    "    }\n",
    ")\n",
    "ffmpeg_d(\n",
    "    {output_audio: None, main_video: None},\n",
    "    {\n",
    "    output_video: ['-y', '-c:v', 'copy', '-c:a', 'aac', '-strict', 'experimental']\n",
    "    }\n",
    ")\n",
    "print('\\\\n'.join(message))\n",
    "      '''\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDAS4aXSTvpY"
   },
   "source": [
    "## CPU ROM 16G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyCKgBF2T04n"
   },
   "outputs": [],
   "source": [
    "!cd /content/index-tts && uv run api.py --model_dir /content/index-tts/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIvMo9H-n-xj"
   },
   "outputs": [],
   "source": [
    "!cd /content/index-tts && uv run script.py --model_dir /content/index-tts/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed39dx8MT4Cj"
   },
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PFy5jkLT5oN"
   },
   "outputs": [],
   "source": [
    "!cd /content/index-tts && uv run api.py --model_dir /content/index-tts/checkpoints --fp16 --deepspeed --cuda_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BhTd4UKn6I1"
   },
   "outputs": [],
   "source": [
    "!cd /content/index-tts && uv run script.py --model_dir /content/index-tts/checkpoints --fp16 --deepspeed --cuda_kernel"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNLTgYbmiHYZyj2mMVq+PdT",
   "collapsed_sections": [
    "HVu_AWQjnu1o",
    "zb7N2tIFUeC3"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
